{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbff1111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:48:39.352978Z",
     "iopub.status.busy": "2025-06-10T07:48:39.352671Z",
     "iopub.status.idle": "2025-06-10T07:48:45.655912Z",
     "shell.execute_reply": "2025-06-10T07:48:45.655152Z"
    },
    "papermill": {
     "duration": 6.309196,
     "end_time": "2025-06-10T07:48:45.657395",
     "exception": false,
     "start_time": "2025-06-10T07:48:39.348199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm # 导入 tqdm 库\n",
    "\n",
    "# 定义实体类型前缀，用于ID映射和区分\n",
    "ENTITY_TYPES = {\n",
    "    'user': 'U_',\n",
    "    'course': 'C_',\n",
    "    'knowledge': 'K_',\n",
    "    'school': 'S_',\n",
    "    'teacher': 'T_',\n",
    "    'major': 'M_'\n",
    "}\n",
    "\n",
    "def load_and_process_data_for_experiment(\n",
    "    data_dir, # 数据文件所在的目录\n",
    "    # 消融实验参数\n",
    "    include_kg_course_knowledge=True, \n",
    "    include_kg_school_course=True,\n",
    "    include_kg_teacher_course=True,\n",
    "    include_kg_kp_prereq=True,\n",
    "    include_kg_knowledge_major=True\n",
    "):\n",
    "    \"\"\"\n",
    "    加载所有原始数据，进行全局ID映射，划分训练/测试交互，并根据配置构建图。\n",
    "    在耗时部分添加了tqdm进度条。\n",
    "    \"\"\"\n",
    "    # --- 1. 加载所有原始数据 ---\n",
    "    print(\"Step 1: Loading raw data files...\")\n",
    "    df_train_uc = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "    df_test_uc = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "    df_knowledge_major = pd.read_csv(os.path.join(data_dir, 'knowledge-major.csv'))\n",
    "    df_course_knowledge = pd.read_csv(os.path.join(data_dir, 'course-knowledge.csv'))\n",
    "    df_school_course = pd.read_csv(os.path.join(data_dir, 'school-course.csv'))\n",
    "    df_teacher_course = pd.read_csv(os.path.join(data_dir, 'teacher-course.csv'))\n",
    "    df_prerequisite_relations = pd.read_csv(os.path.join(data_dir, 'prerequisite_relations.csv'))\n",
    "    print(\"Data files loaded successfully.\")\n",
    "\n",
    "    # --- 2. 收集所有唯一实体并分配全局ID ---\n",
    "    print(\"\\nStep 2: Building global entity-to-ID mapping...\")\n",
    "    entity_to_id = {}\n",
    "    id_counter = 0\n",
    "\n",
    "    def get_global_id(entity_str):\n",
    "        nonlocal id_counter\n",
    "        if entity_str not in entity_to_id:\n",
    "            entity_to_id[entity_str] = id_counter\n",
    "            id_counter += 1\n",
    "        return entity_to_id[entity_str]\n",
    "\n",
    "    # 使用tqdm来显示实体收集的进度\n",
    "    all_entities = [\n",
    "        (\"Users\", pd.concat([df_train_uc['user'], df_test_uc['user']]).unique()),\n",
    "        (\"Courses\", pd.concat([df_train_uc['course'], df_test_uc['course'], \n",
    "                               df_course_knowledge['course'], df_school_course['course'], \n",
    "                               df_teacher_course['course']]).unique()),\n",
    "        (\"Knowledge Points\", pd.concat([df_knowledge_major['knowledge'], df_course_knowledge['knowledge'], \n",
    "                                        df_prerequisite_relations['knowledge1'], df_prerequisite_relations['knowledge2']]).unique()),\n",
    "        (\"Schools\", df_school_course['school'].unique()),\n",
    "        (\"Teachers\", df_teacher_course['teacher'].unique()),\n",
    "        (\"Majors\", df_knowledge_major['major'].unique())\n",
    "    ]\n",
    "    \n",
    "    for name, entities in all_entities:\n",
    "        for entity in tqdm(entities, desc=f\"  Processing {name}\"):\n",
    "            get_global_id(entity)\n",
    "\n",
    "    num_nodes = id_counter # 总节点数\n",
    "    print(f\"Global ID mapping built. Total unique nodes: {num_nodes}\")\n",
    "\n",
    "    # 记录用户和课程的全局ID范围\n",
    "    user_ids = sorted([v for k,v in entity_to_id.items() if k.startswith(ENTITY_TYPES['user'])])\n",
    "    course_ids = sorted([v for k,v in entity_to_id.items() if k.startswith(ENTITY_TYPES['course'])])\n",
    "    user_ids_global_range = (min(user_ids), max(user_ids) + 1) if user_ids else (0,0)\n",
    "    course_ids_global_range = (min(course_ids), max(course_ids) + 1) if course_ids else (0,0)\n",
    "\n",
    "    # --- 3. 构建训练集和测试集的用户-课程交互（已ID化） ---\n",
    "    print(\"\\nStep 3: Processing train and test interactions...\")\n",
    "    train_interactions = []\n",
    "    user_positive_items_train = {}\n",
    "    for _, row in tqdm(df_train_uc.iterrows(), total=df_train_uc.shape[0], desc=\"  Processing train.csv\"):\n",
    "        u_id = get_global_id(row['user'])\n",
    "        c_id = get_global_id(row['course'])\n",
    "        train_interactions.append((u_id, c_id))\n",
    "        user_positive_items_train.setdefault(u_id, set()).add(c_id)\n",
    "\n",
    "    test_interactions = []\n",
    "    user_positive_items_test = {}\n",
    "    for _, row in tqdm(df_test_uc.iterrows(), total=df_test_uc.shape[0], desc=\"  Processing test.csv\"):\n",
    "        u_id = get_global_id(row['user'])\n",
    "        c_id = get_global_id(row['course'])\n",
    "        test_interactions.append((u_id, c_id))\n",
    "        user_positive_items_test.setdefault(u_id, []).append(c_id)\n",
    "    print(\"Interactions processed.\")\n",
    "\n",
    "    # --- 4. 构建用于LightGCN的图的 edge_index ---\n",
    "    print(\"\\nStep 4: Building graph edges...\")\n",
    "    edges = []\n",
    "    # 4.1 添加训练集的用户-课程交互边 (双向)\n",
    "    for u_id, c_id in tqdm(train_interactions, desc=\"  Adding user-course edges\"):\n",
    "        edges.append((u_id, c_id))\n",
    "        edges.append((c_id, u_id))\n",
    "\n",
    "    # 4.2 根据消融实验配置添加知识图谱边 (双向)\n",
    "    if include_kg_course_knowledge:\n",
    "        for _, row in tqdm(df_course_knowledge.iterrows(), total=df_course_knowledge.shape[0], desc=\"  Adding course-knowledge edges\"):\n",
    "            c_id = get_global_id(row['course'])\n",
    "            k_id = get_global_id(row['knowledge'])\n",
    "            edges.append((c_id, k_id))\n",
    "            edges.append((k_id, c_id))\n",
    "\n",
    "    if include_kg_school_course:\n",
    "        for _, row in tqdm(df_school_course.iterrows(), total=df_school_course.shape[0], desc=\"  Adding school-course edges\"):\n",
    "            s_id = get_global_id(row['school'])\n",
    "            c_id = get_global_id(row['course'])\n",
    "            edges.append((s_id, c_id))\n",
    "            edges.append((c_id, s_id))\n",
    "\n",
    "    if include_kg_teacher_course:\n",
    "        for _, row in tqdm(df_teacher_course.iterrows(), total=df_teacher_course.shape[0], desc=\"  Adding teacher-course edges\"):\n",
    "            t_id = get_global_id(row['teacher'])\n",
    "            c_id = get_global_id(row['course'])\n",
    "            edges.append((t_id, c_id))\n",
    "            edges.append((c_id, t_id))\n",
    "\n",
    "    if include_kg_kp_prereq:\n",
    "        for _, row in tqdm(df_prerequisite_relations.iterrows(), total=df_prerequisite_relations.shape[0], desc=\"  Adding prerequisite edges\"):\n",
    "            kp1_id = get_global_id(row['knowledge1'])\n",
    "            kp2_id = get_global_id(row['knowledge2'])\n",
    "            edges.append((kp1_id, kp2_id))\n",
    "            edges.append((kp2_id, kp1_id))\n",
    "\n",
    "    if include_kg_knowledge_major:\n",
    "        for _, row in tqdm(df_knowledge_major.iterrows(), total=df_knowledge_major.shape[0], desc=\"  Adding knowledge-major edges\"):\n",
    "            k_id = get_global_id(row['knowledge'])\n",
    "            m_id = get_global_id(row['major'])\n",
    "            edges.append((k_id, m_id))\n",
    "            edges.append((m_id, k_id))\n",
    "            \n",
    "    print(\"Graph edges built. Converting to tensor...\")\n",
    "    src_nodes, dst_nodes = zip(*edges)\n",
    "    edge_index = torch.tensor([src_nodes, dst_nodes], dtype=torch.long)\n",
    "    print(\"Data loading and processing complete.\")\n",
    "\n",
    "    return num_nodes, edge_index, train_interactions, test_interactions, \\\n",
    "           user_ids_global_range, course_ids_global_range, \\\n",
    "           user_positive_items_train, user_positive_items_test, entity_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fec0dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:48:45.664747Z",
     "iopub.status.busy": "2025-06-10T07:48:45.664350Z",
     "iopub.status.idle": "2025-06-10T07:48:52.500499Z",
     "shell.execute_reply": "2025-06-10T07:48:52.499895Z"
    },
    "papermill": {
     "duration": 6.84135,
     "end_time": "2025-06-10T07:48:52.501772",
     "exception": false,
     "start_time": "2025-06-10T07:48:45.660422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# 从 PyTorch Geometric 导入辅助函数\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "\n",
    "class LightGCN(nn.Module):\n",
    "    \"\"\"\n",
    "    LightGCN模型实现。\n",
    "    LightGCN通过移除GCN中的特征变换和非线性激活函数，简化图卷积过程。\n",
    "    它在异构图上进行消息传播，学习所有节点的Embedding。\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes, embedding_dim, num_layers):\n",
    "        \"\"\"\n",
    "        初始化LightGCN模型。\n",
    "\n",
    "        Args:\n",
    "            num_nodes (int): 图中所有节点的总数（用户+课程+知识点+学校+老师+专业）。\n",
    "            embedding_dim (int): 节点Embedding的维度。\n",
    "            num_layers (int): LightGCN的消息传播层数。\n",
    "        \"\"\"\n",
    "        super(LightGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # 为所有节点初始化Embedding向量\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        # 使用Xavier均匀分布初始化Embedding权重，有助于训练稳定\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "    def forward(self, edge_index):\n",
    "        \"\"\"\n",
    "        执行LightGCN的消息传播过程。\n",
    "\n",
    "        Args:\n",
    "            edge_index (torch.LongTensor): 图的边索引，格式为 (2, num_edges)。\n",
    "                                            包含了所有异构边。\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 所有节点的最终学习到的Embedding矩阵，形状为 (num_nodes, embedding_dim)。\n",
    "        \"\"\"\n",
    "        # 1. 添加自循环并标准化邻接矩阵 (LightGCN的A_hat = A + I)\n",
    "        # add_self_loops 函数会向 edge_index 添加自循环，并返回新的边索引。\n",
    "        edge_index_norm, _ = add_self_loops(edge_index, num_nodes=self.num_nodes)\n",
    "\n",
    "        # 计算度矩阵的逆平方根，用于归一化\n",
    "        row, col = edge_index_norm\n",
    "        deg = degree(col, self.num_nodes, dtype=row.dtype) # 计算每个节点的度\n",
    "        deg_inv_sqrt = deg.pow(-0.5) # 度^-0.5\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0 # 避免对度为0的节点进行无穷大操作\n",
    "\n",
    "        # 计算归一化因子 (D_hat)^(-1/2) * (D_hat)^(-1/2)\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # 构建稀疏邻接矩阵。PyTorch Geometric在内部会更高效地处理图结构。\n",
    "        # 这里手动构建是为了更清晰地展示LightGCN的核心传播步骤。\n",
    "        adj_matrix = torch.sparse_coo_tensor(\n",
    "            edge_index_norm, norm, (self.num_nodes, self.num_nodes), \n",
    "            dtype=torch.float32, device=self.embedding.weight.device\n",
    "        )\n",
    "        \n",
    "        # 2. 消息传播\n",
    "        # all_embeddings 列表存储每一层传播后的节点Embedding\n",
    "        all_embeddings = [self.embedding.weight] # L0 层的Embedding就是初始Embedding\n",
    "\n",
    "        for layer in range(self.num_layers):\n",
    "            # E^(l+1) = (Normalized_Adj_Matrix) * E^(l)\n",
    "            # 即每个节点从其邻居聚合信息\n",
    "            current_embeddings = torch.sparse.mm(adj_matrix, all_embeddings[-1])\n",
    "            all_embeddings.append(current_embeddings)\n",
    "\n",
    "        # 3. 最终Embedding聚合 (所有层级Embedding的平均)\n",
    "        # LightGCN的最终Embedding是所有层（包括初始层）Embedding的平均。\n",
    "        # torch.stack 将列表中的张量沿新维度堆叠起来 (num_layers+1, num_nodes, embedding_dim)\n",
    "        # torch.mean 沿第一个维度（层维度）取平均，得到 (num_nodes, embedding_dim)\n",
    "        final_embeddings = torch.mean(torch.stack(all_embeddings, dim=1), dim=1)\n",
    "        \n",
    "        return final_embeddings\n",
    "\n",
    "    # 辅助方法：在评估阶段，可以根据全局ID范围获取特定类型的Embedding\n",
    "    # 通常不需要直接调用，因为评估函数直接通过全局ID索引 final_embeddings\n",
    "    def get_user_course_embeddings(self, final_embeddings, user_ids_global_range, course_ids_global_range):\n",
    "        user_embeddings = final_embeddings[user_ids_global_range[0]:user_ids_global_range[1]]\n",
    "        course_embeddings = final_embeddings[course_ids_global_range[0]:course_ids_global_range[1]]\n",
    "        return user_embeddings, course_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "467db641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:48:52.508707Z",
     "iopub.status.busy": "2025-06-10T07:48:52.508178Z",
     "iopub.status.idle": "2025-06-10T07:48:52.524622Z",
     "shell.execute_reply": "2025-06-10T07:48:52.524105Z"
    },
    "papermill": {
     "duration": 0.020855,
     "end_time": "2025-06-10T07:48:52.525569",
     "exception": false,
     "start_time": "2025-06-10T07:48:52.504714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, edge_index, train_interactions,\n",
    "                user_ids_global_range, course_ids_global_range,\n",
    "                num_epochs, batch_size, learning_rate, device):\n",
    "    \"\"\"\n",
    "    训练LightGCN模型，并返回每个Epoch的损失。\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    model.to(device)\n",
    "    edge_index = edge_index.to(device)\n",
    "\n",
    "    all_course_global_ids = list(range(course_ids_global_range[0], course_ids_global_range[1]))\n",
    "    \n",
    "    print(\"Building training user-positive-items map...\")\n",
    "    user_positive_items = {}\n",
    "    for u_id, c_id in tqdm(train_interactions, desc=\"  Building map\"):\n",
    "        user_positive_items.setdefault(u_id, set()).add(c_id)\n",
    "\n",
    "    # 新增：用于记录每个Epoch的损失\n",
    "    epoch_losses = []\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        random.shuffle(train_interactions)\n",
    "        \n",
    "        # 遍历训练交互数据，按批次进行训练\n",
    "        batch_iterator = tqdm(range(0, len(train_interactions), batch_size), desc=f\"Epoch {epoch+1}\")\n",
    "        for i in batch_iterator:\n",
    "            batch_interactions = train_interactions[i : i + batch_size]\n",
    "            \n",
    "            batch_users, batch_pos_courses, batch_neg_courses = [], [], []\n",
    "            for u_id, pos_c_id in batch_interactions:\n",
    "                batch_users.append(u_id)\n",
    "                batch_pos_courses.append(pos_c_id)\n",
    "                neg_c_id = random.choice(all_course_global_ids)\n",
    "                while neg_c_id in user_positive_items.get(u_id, set()):\n",
    "                    neg_c_id = random.choice(all_course_global_ids)\n",
    "                batch_neg_courses.append(neg_c_id)\n",
    "\n",
    "            batch_users_tensor = torch.tensor(batch_users, dtype=torch.long).to(device)\n",
    "            batch_pos_courses_tensor = torch.tensor(batch_pos_courses, dtype=torch.long).to(device)\n",
    "            batch_neg_courses_tensor = torch.tensor(batch_neg_courses, dtype=torch.long).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            final_embeddings = model(edge_index)\n",
    "            \n",
    "            user_embeddings = final_embeddings[batch_users_tensor]\n",
    "            pos_course_embeddings = final_embeddings[batch_pos_courses_tensor]\n",
    "            neg_course_embeddings = final_embeddings[batch_neg_courses_tensor]\n",
    "\n",
    "            pos_scores = (user_embeddings * pos_course_embeddings).sum(dim=1)\n",
    "            neg_scores = (user_embeddings * neg_course_embeddings).sum(dim=1)\n",
    "            loss = -F.logsigmoid(pos_scores - neg_scores).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            # 在tqdm进度条上动态显示当前批次的损失\n",
    "            batch_iterator.set_postfix(loss=loss.item())\n",
    "        \n",
    "        avg_epoch_loss = total_loss / (len(train_interactions) / batch_size)\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {avg_epoch_loss:.4f}\")\n",
    "    \n",
    "    print(\"Training finished.\")\n",
    "\n",
    "    # 返回模型、最终Embedding和损失历史\n",
    "    return model, final_embeddings, epoch_losses\n",
    "\n",
    "# evaluate_model 函数保持不变\n",
    "# ... (将 evaluate_model 函数的完整代码粘贴到这里) ...\n",
    "def evaluate_model(model, edge_index, final_embeddings, \n",
    "                   test_interactions,\n",
    "                   user_ids_global_range, course_ids_global_range,\n",
    "                   user_positive_items_train, # 用户在训练集中已交互的课程\n",
    "                   user_positive_items_test,  # 用户在测试集中已交互的课程\n",
    "                   k_values=[10, 20], num_neg_samples=99, device='cpu'):\n",
    "    model.eval() # 设置模型为评估模式 (禁用 dropout 等)\n",
    "    \n",
    "    # 获取所有课程的全局ID列表，用于负采样\n",
    "    course_start_id, course_end_id = course_ids_global_range\n",
    "    all_course_global_ids = list(range(course_start_id, course_end_id))\n",
    "\n",
    "    # 获取在测试集中有实际交互的用户列表，只评估这些用户\n",
    "    test_users = list(user_positive_items_test.keys())\n",
    "    \n",
    "    hr_scores = {k: [] for k in k_values}\n",
    "    ndcg_scores = {k: [] for k in k_values}\n",
    "\n",
    "    print(\"Starting evaluation...\")\n",
    "    with torch.no_grad(): # 在评估阶段不需要计算梯度，节省内存和时间\n",
    "        \n",
    "        for u_id in tqdm(test_users, desc=\"Evaluating\"):\n",
    "            # 1. 获取该用户在测试集中的所有真实正样本\n",
    "            positive_courses_in_test = user_positive_items_test.get(u_id, [])\n",
    "            if not positive_courses_in_test: \n",
    "                continue # 如果用户在测试集中没有正样本，则跳过\n",
    "\n",
    "            # 2. 获取用户在训练集中已交互过的所有课程 (用于负样本排除)\n",
    "            interacted_courses_train = user_positive_items_train.get(u_id, set())\n",
    "            \n",
    "            # 3. 构建候选推荐列表：包含测试集正样本 + 负样本\n",
    "            candidate_courses_global_ids = []\n",
    "            candidate_courses_global_ids.extend(positive_courses_in_test) # 加入测试集中的所有正样本\n",
    "\n",
    "            # 采样负样本\n",
    "            neg_candidates = []\n",
    "            # 用户所有已知的交互 (训练集正样本 + 测试集正样本)\n",
    "            current_user_all_interacted_courses = interacted_courses_train.union(set(positive_courses_in_test))\n",
    "\n",
    "            while len(neg_candidates) < num_neg_samples:\n",
    "                sampled_neg_id = random.choice(all_course_global_ids) # 从所有课程中随机选\n",
    "                # 确保采样到的负样本：不在用户所有已交互课程中\n",
    "                if sampled_neg_id not in current_user_all_interacted_courses:\n",
    "                    neg_candidates.append(sampled_neg_id)\n",
    "            \n",
    "            candidate_courses_global_ids.extend(neg_candidates)\n",
    "            random.shuffle(candidate_courses_global_ids) # 打乱顺序，避免任何偏差\n",
    "\n",
    "            # 将候选课程IDs转换为PyTorch Tensor，并移动到指定设备\n",
    "            candidate_courses_tensor = torch.tensor(candidate_courses_global_ids, dtype=torch.long).to(device)\n",
    "\n",
    "            # 4. 预测分数\n",
    "            user_emb = final_embeddings[u_id].unsqueeze(0) # (1, embedding_dim)\n",
    "            candidate_embs = final_embeddings[candidate_courses_tensor] # (num_candidates, embedding_dim)\n",
    "            \n",
    "            scores = torch.matmul(user_emb, candidate_embs.T).squeeze(0) # (num_candidates,)\n",
    "\n",
    "            # 5. 排序并获取Top-K推荐列表\n",
    "            _, top_indices = torch.topk(scores, k=max(k_values))\n",
    "            predicted_global_ids = candidate_courses_tensor[top_indices].cpu().numpy()\n",
    "\n",
    "            # 6. 计算HR和NDCG\n",
    "            for k in k_values:\n",
    "                # HR@K (Hit Ratio)\n",
    "                hit = False\n",
    "                for pos_c_id in positive_courses_in_test:\n",
    "                    if pos_c_id in predicted_global_ids[:k]: \n",
    "                        hit = True\n",
    "                        break \n",
    "                hr_scores[k].append(1 if hit else 0)\n",
    "\n",
    "                # NDCG@K (Normalized Discounted Cumulative Gain)\n",
    "                dcg = 0.0 # Discounted Cumulative Gain\n",
    "                \n",
    "                # 计算DCG：遍历Top-K推荐列表\n",
    "                for rank, pred_id in enumerate(predicted_global_ids[:k]):\n",
    "                    if pred_id in positive_courses_in_test:\n",
    "                        dcg += 1.0 / np.log2(rank + 2) \n",
    "                \n",
    "                # 计算IDCG (Ideal DCG)：理想情况下的最大DCG\n",
    "                idcg = 0.0\n",
    "                num_relevant_in_k = min(len(positive_courses_in_test), k)\n",
    "                for rank in range(num_relevant_in_k):\n",
    "                    idcg += 1.0 / np.log2(rank + 2)\n",
    "\n",
    "                if idcg == 0: \n",
    "                    ndcg_scores[k].append(0.0)\n",
    "                else:\n",
    "                    ndcg_scores[k].append(dcg / idcg)\n",
    "    \n",
    "    # 7. 汇总所有用户的指标\n",
    "    avg_hr = {k: np.mean(hr_scores[k]) for k in k_values}\n",
    "    avg_ndcg = {k: np.mean(ndcg_scores[k]) for k in k_values}\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for k in k_values:\n",
    "        print(f\"HR@{k}: {avg_hr[k]:.4f}\")\n",
    "        print(f\"NDCG@{k}: {avg_ndcg[k]:.4f}\")\n",
    "\n",
    "    return avg_hr, avg_ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e622ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T07:48:52.531832Z",
     "iopub.status.busy": "2025-06-10T07:48:52.531624Z",
     "iopub.status.idle": "2025-06-10T11:00:11.335842Z",
     "shell.execute_reply": "2025-06-10T11:00:11.334967Z"
    },
    "papermill": {
     "duration": 11478.808647,
     "end_time": "2025-06-10T11:00:11.336926",
     "exception": false,
     "start_time": "2025-06-10T07:48:52.528279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 版本1 全部实验未调参\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 检查是否有可用的GPU，并设置设备\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 定义数据文件所在的目录\n",
    "    data_dir = '/kaggle/input/data12/data/' \n",
    "    results_dir = '/kaggle/working/data12results/'\n",
    "    os.makedirs(results_dir, exist_ok=True) # 确保结果目录存在\n",
    "    # 2. 模型和训练超参数\n",
    "    embedding_dim = 32  # 节点Embedding的维度\n",
    "    num_layers = 8      # LightGCN的消息传播层数\n",
    "    num_epochs = 50     # 训练轮数\n",
    "    batch_size = 2048   # BPR损失计算的批处理大小\n",
    "    learning_rate = 0.001 # 优化器学习率\n",
    "\n",
    "    # --- 消融实验的配置字典 ---\n",
    "    experiment_configs = {\n",
    "        \"Baseline_LightGCN\": { # 仅用户-课程交互\n",
    "            \"include_kg_course_knowledge\": False,\n",
    "            \"include_kg_school_course\": False,\n",
    "            \"include_kg_teacher_course\": False,\n",
    "            \"include_kg_kp_prereq\": False,\n",
    "            \"include_kg_knowledge_major\": False\n",
    "        },\n",
    "        \"LightGCN_plus_CourseKnowledge\": { # 加入课程-知识点\n",
    "            \"include_kg_course_knowledge\": True,\n",
    "            \"include_kg_school_course\": False,\n",
    "            \"include_kg_teacher_course\": False,\n",
    "            \"include_kg_kp_prereq\": False,\n",
    "            \"include_kg_knowledge_major\": False\n",
    "        },\n",
    "        \"LightGCN_plus_CourseKnowledge_School\": { # 加入学校-课程\n",
    "            \"include_kg_course_knowledge\": True,\n",
    "            \"include_kg_school_course\": True,\n",
    "            \"include_kg_teacher_course\": False,\n",
    "            \"include_kg_kp_prereq\": False,\n",
    "            \"include_kg_knowledge_major\": False\n",
    "        },\n",
    "        \"LightGCN_plus_CourseKnowledge_School_Teacher\": { # 加入老师-课程\n",
    "            \"include_kg_course_knowledge\": True,\n",
    "            \"include_kg_school_course\": True,\n",
    "            \"include_kg_teacher_course\": True,\n",
    "            \"include_kg_kp_prereq\": False,\n",
    "            \"include_kg_knowledge_major\": False\n",
    "        },\n",
    "        \"LightGCN_plus_CourseKnowledge_School_Teacher_KPMajor\": { # 加入知识点-专业\n",
    "            \"include_kg_course_knowledge\": True,\n",
    "            \"include_kg_school_course\": True,\n",
    "            \"include_kg_teacher_course\": True,\n",
    "            \"include_kg_kp_prereq\": False,\n",
    "            \"include_kg_knowledge_major\": True\n",
    "        },\n",
    "        \"LightGCN_Full_KG\": { # 最终模型 (包含所有知识图谱边)\n",
    "            \"include_kg_course_knowledge\": True,\n",
    "            \"include_kg_school_course\": True,\n",
    "            \"include_kg_teacher_course\": True,\n",
    "            \"include_kg_kp_prereq\": True,\n",
    "            \"include_kg_knowledge_major\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    results_summary = {} # 用于存储所有实验的最终结果\n",
    "\n",
    "    # --- 控制运行单个实验的开关 ---\n",
    "    # 将此变量设置为你希望运行的实验名称（键），例如 \"LightGCN_Full_KG\"\n",
    "    # 如果设置为 None 或 \"\"，则会运行 'experiment_configs' 中定义的所有实验。\n",
    "    # run_specific_experiment = \"LightGCN_Full_KG\" # 默认先跑完整模型\n",
    "    # run_specific_experiment = \"Baseline_LightGCN\" # 也可以先跑Baseline\n",
    "    run_specific_experiment = None # 设置为 None 会运行所有实验\n",
    "\n",
    "    if run_specific_experiment:\n",
    "        print(f\"Running ONLY the experiment: {run_specific_experiment}\\n\")\n",
    "    else:\n",
    "        print(\"Running ALL defined experiments.\\n\")\n",
    "\n",
    "    for exp_name, config in experiment_configs.items():\n",
    "        if run_specific_experiment and exp_name != run_specific_experiment:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n--- Running Experiment: {exp_name} ---\")\n",
    "        \n",
    "        # 1. 数据加载和处理\n",
    "        num_nodes, edge_index, train_interactions, test_interactions, \\\n",
    "        user_ids_global_range, course_ids_global_range, \\\n",
    "        user_positive_items_train, user_positive_items_test, entity_to_id = \\\n",
    "            load_and_process_data_for_experiment(data_dir, **config)\n",
    "\n",
    "        print(f\"Graph nodes: {num_nodes}, edges: {edge_index.shape[1] // 2}\")\n",
    "        print(f\"Training interactions: {len(train_interactions)}, Test interactions: {len(test_interactions)}\")\n",
    "\n",
    "        # 3. 初始化模型\n",
    "        model = LightGCN(num_nodes, embedding_dim, num_layers)\n",
    "        print(f\"Model initialized for {exp_name}.\")\n",
    "\n",
    "        # 4. 训练模型\n",
    "        trained_model, final_embeddings, epoch_losses = train_model(\n",
    "            model, edge_index, train_interactions,\n",
    "            user_ids_global_range, course_ids_global_range,\n",
    "            num_epochs, batch_size, learning_rate, device\n",
    "        )\n",
    "\n",
    "        # --- 保存训练过程和结果 ---\n",
    "        # 创建当前实验的结果子目录\n",
    "        exp_results_dir = os.path.join(results_dir, exp_name)\n",
    "        os.makedirs(exp_results_dir, exist_ok=True)\n",
    "        \n",
    "        # 4.1 保存模型权重\n",
    "        model_save_path = os.path.join(exp_results_dir, 'model_weights.pt')\n",
    "        torch.save(trained_model.state_dict(), model_save_path)\n",
    "        print(f\"Model weights for '{exp_name}' saved to {model_save_path}\")\n",
    "\n",
    "        # 4.2 保存损失历史\n",
    "        loss_df = pd.DataFrame({'epoch': range(1, num_epochs + 1), 'loss': epoch_losses})\n",
    "        loss_save_path = os.path.join(exp_results_dir, 'loss_history.csv')\n",
    "        loss_df.to_csv(loss_save_path, index=False)\n",
    "        print(f\"Loss history for '{exp_name}' saved to {loss_save_path}\")\n",
    "        \n",
    "        # 4.3 保存最终Embedding\n",
    "        embedding_save_path = os.path.join(exp_results_dir, 'final_embeddings.pt')\n",
    "        torch.save(final_embeddings, embedding_save_path)\n",
    "        print(f\"Final embeddings for '{exp_name}' saved to {embedding_save_path}\")\n",
    "        \n",
    "        # 4.4 保存ID映射 (非常重要，用于解释Embedding)\n",
    "        id_map_save_path = os.path.join(exp_results_dir, 'entity_to_id.json')\n",
    "        with open(id_map_save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(entity_to_id, f, ensure_ascii=False, indent=4)\n",
    "        print(f\"Entity-to-ID map for '{exp_name}' saved to {id_map_save_path}\")\n",
    "\n",
    "        # 5. 评估模型\n",
    "        hr_results, ndcg_results = evaluate_model(\n",
    "            trained_model, edge_index, final_embeddings, \n",
    "            test_interactions,\n",
    "            user_ids_global_range, course_ids_global_range,\n",
    "            user_positive_items_train, user_positive_items_test,\n",
    "            k_values=[10, 20], device=device\n",
    "        )\n",
    "        \n",
    "        results_summary[exp_name] = {\n",
    "            \"HR@10\": hr_results[10], \"NDCG@10\": ndcg_results[10],\n",
    "            \"HR@20\": hr_results[20], \"NDCG@20\": ndcg_results[20]\n",
    "        }\n",
    "    \n",
    "    # 打印并保存所有实验的最终结果汇总\n",
    "    print(\"\\n\\n--- All Experiments Summary ---\")\n",
    "    if not results_summary:\n",
    "        print(\"No experiments were run.\")\n",
    "    else:\n",
    "        summary_df = pd.DataFrame.from_dict(results_summary, orient='index')\n",
    "        summary_df.index.name = 'Experiment_Name'\n",
    "        print(summary_df)\n",
    "        \n",
    "        # 保存汇总结果到CSV文件\n",
    "        summary_save_path = os.path.join(results_dir, 'experiment_summary.csv')\n",
    "        summary_df.to_csv(summary_save_path)\n",
    "        print(f\"\\nExperiment summary saved to {summary_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2fe46b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:00:27.319022Z",
     "iopub.status.busy": "2025-06-10T11:00:27.318651Z",
     "iopub.status.idle": "2025-06-10T11:00:27.324036Z",
     "shell.execute_reply": "2025-06-10T11:00:27.323500Z"
    },
    "papermill": {
     "duration": 8.035669,
     "end_time": "2025-06-10T11:00:27.325043",
     "exception": false,
     "start_time": "2025-06-10T11:00:19.289374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#版本2 网格调参\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools # 导入itertools来轻松生成笛卡尔积\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 定义数据文件和结果输出的目录\n",
    "    data_dir = '/kaggle/input/data12/data/' \n",
    "    results_dir = '/kaggle/working/hyperparameter_tuning_results/' # 为调优结果创建一个新目录\n",
    "    os.makedirs(results_dir, exist_ok=True) # 确保结果目录存在\n",
    "\n",
    "    # --- 1. 定义超参数网格 ---\n",
    "    param_grid = {\n",
    "        'num_layers': [2, 3, 4],\n",
    "        'embedding_dim': [32, 64, 128], # 为了节省时间，先不跑256，如果128效果好再加\n",
    "        'learning_rate': [0.01, 0.005, 0.001]\n",
    "    }\n",
    "    \n",
    "    # 固定的训练参数\n",
    "    num_epochs = 50\n",
    "    batch_size = 2048\n",
    "\n",
    "    # --- 2. 准备数据加载所需的固定配置 ---\n",
    "    # 我们只对 Full_KG 模型进行调优\n",
    "    full_kg_config = {\n",
    "        \"include_kg_course_knowledge\": True,\n",
    "        \"include_kg_school_course\": True,\n",
    "        \"include_kg_teacher_course\": True,\n",
    "        \"include_kg_kp_prereq\": True,\n",
    "        \"include_kg_knowledge_major\": True\n",
    "    }\n",
    "    \n",
    "    # --- 3. 数据只加载一次，因为图结构是固定的 (Full KG) ---\n",
    "    print(\"--- Loading and Processing Data for Full KG Model (once) ---\")\n",
    "    num_nodes, edge_index, train_interactions, test_interactions, \\\n",
    "    user_ids_global_range, course_ids_global_range, \\\n",
    "    user_positive_items_train, user_positive_items_test, entity_to_id = \\\n",
    "        load_and_process_data_for_experiment(data_dir, **full_kg_config)\n",
    "    \n",
    "    print(f\"Data loaded. Graph nodes: {num_nodes}, edges: {edge_index.shape[1] // 2}\")\n",
    "    \n",
    "    # --- 4. 网格搜索主循环 ---\n",
    "    results_summary = {}\n",
    "    \n",
    "    # 生成所有超参数组合的笛卡尔积\n",
    "    param_combinations = list(itertools.product(\n",
    "        param_grid['num_layers'],\n",
    "        param_grid['embedding_dim'],\n",
    "        param_grid['learning_rate']\n",
    "    ))\n",
    "    \n",
    "    total_experiments = len(param_combinations)\n",
    "    print(f\"\\n--- Starting Grid Search for {total_experiments} Hyperparameter Combinations ---\")\n",
    "\n",
    "    for i, (num_layers, embedding_dim, learning_rate) in enumerate(param_combinations):\n",
    "        \n",
    "        # 动态生成实验名称\n",
    "        exp_name = f\"L{num_layers}_D{embedding_dim}_LR{learning_rate}\"\n",
    "        \n",
    "        print(f\"\\n--- Running Experiment {i+1}/{total_experiments}: {exp_name} ---\")\n",
    "        \n",
    "        # 4.1 初始化模型 (每次循环都重新初始化)\n",
    "        model = LightGCN(num_nodes, embedding_dim, num_layers)\n",
    "        print(f\"Model initialized: layers={num_layers}, dim={embedding_dim}, lr={learning_rate}\")\n",
    "\n",
    "        # 4.2 训练模型\n",
    "        trained_model, final_embeddings, epoch_losses = train_model(\n",
    "            model, edge_index, train_interactions,\n",
    "            user_ids_global_range, course_ids_global_range,\n",
    "            num_epochs, batch_size, learning_rate, device\n",
    "        )\n",
    "\n",
    "        # 4.3 保存训练过程和结果\n",
    "        exp_results_dir = os.path.join(results_dir, exp_name)\n",
    "        os.makedirs(exp_results_dir, exist_ok=True)\n",
    "        \n",
    "        # 保存模型权重\n",
    "        model_save_path = os.path.join(exp_results_dir, 'model_weights.pt')\n",
    "        torch.save(trained_model.state_dict(), model_save_path)\n",
    "        \n",
    "        # 保存损失历史\n",
    "        loss_df = pd.DataFrame({'epoch': range(1, num_epochs + 1), 'loss': epoch_losses})\n",
    "        loss_save_path = os.path.join(exp_results_dir, 'loss_history.csv')\n",
    "        loss_df.to_csv(loss_save_path, index=False)\n",
    "        print(f\"Results for '{exp_name}' saved to {exp_results_dir}\")\n",
    "\n",
    "        # 4.4 评估模型\n",
    "        hr_results, ndcg_results = evaluate_model(\n",
    "            trained_model, edge_index, final_embeddings, \n",
    "            test_interactions,\n",
    "            user_ids_global_range, course_ids_global_range,\n",
    "            user_positive_items_train, user_positive_items_test,\n",
    "            k_values=[10, 20], device=device\n",
    "        )\n",
    "        \n",
    "        # 4.5 记录结果到汇总字典\n",
    "        results_summary[exp_name] = {\n",
    "            \"num_layers\": num_layers,\n",
    "            \"embedding_dim\": embedding_dim,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"HR@10\": hr_results[10], \"NDCG@10\": ndcg_results[10],\n",
    "            \"HR@20\": hr_results[20], \"NDCG@20\": ndcg_results[20]\n",
    "        }\n",
    "    \n",
    "    # 5. 打印并保存所有实验的最终结果汇总\n",
    "    print(\"\\n\\n--- Grid Search Summary ---\")\n",
    "    if not results_summary:\n",
    "        print(\"No experiments were run.\")\n",
    "    else:\n",
    "        summary_df = pd.DataFrame.from_dict(results_summary, orient='index')\n",
    "        summary_df = summary_df.sort_values(by=\"NDCG@10\", ascending=False) # 按NDCG@10降序排序\n",
    "        print(summary_df)\n",
    "        \n",
    "        # 保存汇总结果到CSV文件\n",
    "        summary_save_path = os.path.join(results_dir, 'grid_search_summary.csv')\n",
    "        summary_df.to_csv(summary_save_path)\n",
    "        print(f\"\\nGrid search summary saved to {summary_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db43156a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T11:00:42.955599Z",
     "iopub.status.busy": "2025-06-10T11:00:42.955048Z",
     "iopub.status.idle": "2025-06-10T11:00:42.960486Z",
     "shell.execute_reply": "2025-06-10T11:00:42.959977Z"
    },
    "papermill": {
     "duration": 7.8555,
     "end_time": "2025-06-10T11:00:42.961534",
     "exception": false,
     "start_time": "2025-06-10T11:00:35.106034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 版本3 探索网络深度\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "import itertools\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # 定义数据文件和结果输出的目录\n",
    "    data_dir = '/kaggle/input/data12/data/'\n",
    "    # 为深度探索实验创建一个新的结果目录\n",
    "    results_dir = '/kaggle/working/depth_exploration_results/'\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    # --- 1. 定义深度探索的参数 ---\n",
    "    # 根据你之前的网格搜索结果，我们找到了最优的 embedding_dim 和 learning_rate\n",
    "    best_embedding_dim = 32\n",
    "    best_learning_rate = 0.001\n",
    "    \n",
    "    # 现在，我们只探索 num_layers 的变化\n",
    "    depth_exploration_grid = {\n",
    "        'num_layers': [14,16], \n",
    "        'embedding_dim': [best_embedding_dim], # 固定为最优值\n",
    "        'learning_rate': [best_learning_rate] # 固定为最优值\n",
    "    }\n",
    "    \n",
    "    # 固定的训练参数\n",
    "    num_epochs = 50\n",
    "    batch_size = 2048 # 使用你之前调参时固定的batch_size\n",
    "\n",
    "    # --- 2. 准备数据加载所需的固定配置 (只在Full KG模型上探索) ---\n",
    "    full_kg_config = {\n",
    "        \"include_kg_course_knowledge\": True, \"include_kg_school_course\": True,\n",
    "        \"include_kg_teacher_course\": True, \"include_kg_kp_prereq\": True,\n",
    "        \"include_kg_knowledge_major\": True\n",
    "    }\n",
    "    \n",
    "    # --- 3. 数据只加载一次 ---\n",
    "    print(\"--- Loading and Processing Data for Full KG Model (once) ---\")\n",
    "    num_nodes, edge_index, train_interactions, test_interactions, \\\n",
    "    user_ids_global_range, course_ids_global_range, \\\n",
    "    user_positive_items_train, user_positive_items_test, entity_to_id = \\\n",
    "        load_and_process_data_for_experiment(data_dir, **full_kg_config)\n",
    "    \n",
    "    print(f\"Data loaded. Graph nodes: {num_nodes}, edges: {edge_index.shape[1] // 2}\")\n",
    "    \n",
    "    # --- 4. 深度探索主循环 ---\n",
    "    results_summary = {}\n",
    "    \n",
    "    # 生成所有超参数组合 (这里实际上只是遍历num_layers)\n",
    "    param_combinations = list(itertools.product(\n",
    "        depth_exploration_grid['num_layers'],\n",
    "        depth_exploration_grid['embedding_dim'],\n",
    "        depth_exploration_grid['learning_rate']\n",
    "    ))\n",
    "    \n",
    "    total_experiments = len(param_combinations)\n",
    "    print(f\"\\n--- Starting Depth Exploration for {total_experiments} Configurations ---\")\n",
    "\n",
    "    for i, (num_layers, embedding_dim, learning_rate) in enumerate(param_combinations):\n",
    "        \n",
    "        # 动态生成实验名称\n",
    "        exp_name = f\"Layers_{num_layers}\" # 实验名称现在只关注层数\n",
    "        \n",
    "        print(f\"\\n--- Running Experiment {i+1}/{total_experiments}: {exp_name} ---\")\n",
    "        \n",
    "        # 4.1 初始化模型\n",
    "        model = LightGCN(num_nodes, embedding_dim, num_layers)\n",
    "        print(f\"Model initialized: layers={num_layers}, dim={embedding_dim}, lr={learning_rate}\")\n",
    "\n",
    "        # 4.2 训练模型\n",
    "        trained_model, final_embeddings, epoch_losses = train_model(\n",
    "            model, edge_index, train_interactions,\n",
    "            user_ids_global_range, course_ids_global_range,\n",
    "            num_epochs, batch_size, learning_rate, device\n",
    "        )\n",
    "\n",
    "        # 4.3 保存训练过程和结果\n",
    "        exp_results_dir = os.path.join(results_dir, exp_name)\n",
    "        os.makedirs(exp_results_dir, exist_ok=True)\n",
    "        \n",
    "        torch.save(trained_model.state_dict(), os.path.join(exp_results_dir, 'model_weights.pt'))\n",
    "        \n",
    "        loss_df = pd.DataFrame({'epoch': range(1, num_epochs + 1), 'loss': epoch_losses})\n",
    "        loss_df.to_csv(os.path.join(exp_results_dir, 'loss_history.csv'), index=False)\n",
    "        print(f\"Results for '{exp_name}' saved to {exp_results_dir}\")\n",
    "\n",
    "        # 4.4 评估模型\n",
    "        hr_results, ndcg_results = evaluate_model(\n",
    "            trained_model, edge_index, final_embeddings, \n",
    "            test_interactions,\n",
    "            user_ids_global_range, course_ids_global_range,\n",
    "            user_positive_items_train, user_positive_items_test,\n",
    "            k_values=[10, 20], device=device\n",
    "        )\n",
    "        \n",
    "        # 4.5 记录结果到汇总字典\n",
    "        results_summary[exp_name] = {\n",
    "            \"num_layers\": num_layers,\n",
    "            \"HR@10\": hr_results[10], \"NDCG@10\": ndcg_results[10],\n",
    "            \"HR@20\": hr_results[20], \"NDCG@20\": ndcg_results[20]\n",
    "        }\n",
    "    \n",
    "    # 5. 打印并保存所有实验的最终结果汇总\n",
    "    print(\"\\n\\n--- Depth Exploration Summary ---\")\n",
    "    if not results_summary:\n",
    "        print(\"No experiments were run.\")\n",
    "    else:\n",
    "        summary_df = pd.DataFrame.from_dict(results_summary, orient='index')\n",
    "        # summary_df = summary_df.sort_values(by=\"num_layers\", ascending=True) # 按层数排序\n",
    "        print(summary_df)\n",
    "        \n",
    "        summary_save_path = os.path.join(results_dir, 'depth_exploration_summary.csv')\n",
    "        summary_df.to_csv(summary_save_path)\n",
    "        print(f\"\\nDepth exploration summary saved to {summary_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7610526,
     "sourceId": 12089604,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11544.159772,
   "end_time": "2025-06-10T11:00:54.226808",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-10T07:48:30.067036",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
